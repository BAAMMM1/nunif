import torch
import torch.nn.functional as F


@torch.no_grad
def sampling(x, tau, n, hard=False):
    sum_z = torch.zeros_like(x, dtype=x.dtype)
    for _ in range(n):
        sum_z += F.gumbel_softmax(x, dim=-1, tau=tau, hard=hard)

    return sum_z / n


def test():
    N = 10000
    x = torch.tensor([[1000., 950., 500., 100., 50., 10., 1., 1e-4]])
    x = (x / x.sum())
    print("prob", [round(p, 4) for p in x.tolist()[0]])
    x = x.log()

    for hard in [False, True]:
        print(f"\nhard={hard}")
        for tau in [10., 5., 2., 1., 1e-2, 1e-4, 1e-6, 1e-8]:
            prob = sampling(x, tau, N, hard=hard).tolist()[0]
            prob = [round(p, 4) for p in prob]
            print(tau, prob)


test()


"""
prob [0.383, 0.3638, 0.1915, 0.0383, 0.0191, 0.0038, 0.0004, 0.0]

hard=False
10.0 [0.1726, 0.1716, 0.161, 0.137, 0.1278, 0.1087, 0.0867, 0.0345]
5.0 [0.2099, 0.2087, 0.1836, 0.134, 0.1168, 0.085, 0.0535, 0.0085]
2.0 [0.289, 0.2807, 0.2124, 0.0991, 0.0734, 0.0342, 0.0111, 0.0001]
1.0 [0.3416, 0.3323, 0.2107, 0.0649, 0.0387, 0.0101, 0.0016, 0.0]
0.01 [0.3849, 0.3693, 0.187, 0.0369, 0.0178, 0.0037, 0.0004, 0.0]
0.0001 [0.3866, 0.3685, 0.1862, 0.0367, 0.0174, 0.0043, 0.0003, 0.0]
1e-06 [0.3801, 0.3677, 0.1881, 0.041, 0.0199, 0.0031, 0.0001, 0.0]
1e-08 [0.3868, 0.3632, 0.1875, 0.0367, 0.021, 0.0043, 0.0005, 0.0]

hard=True
10.0 [0.3723, 0.3717, 0.191, 0.0407, 0.0193, 0.0044, 0.0006, 0.0]
5.0 [0.3805, 0.3639, 0.1919, 0.0392, 0.0206, 0.0036, 0.0003, 0.0]
2.0 [0.3915, 0.36, 0.1871, 0.0378, 0.0193, 0.0039, 0.0004, 0.0]
1.0 [0.382, 0.3677, 0.1851, 0.0395, 0.0213, 0.0043, 0.0001, 0.0]
0.01 [0.3845, 0.3634, 0.191, 0.0395, 0.0177, 0.0035, 0.0004, 0.0]
0.0001 [0.3786, 0.373, 0.1873, 0.038, 0.0191, 0.0038, 0.0002, 0.0]
1e-06 [0.3778, 0.3672, 0.191, 0.0406, 0.0198, 0.0033, 0.0003, 0.0]
1e-08 [0.3863, 0.362, 0.1869, 0.0418, 0.0182, 0.0044, 0.0004, 0.0]
"""
